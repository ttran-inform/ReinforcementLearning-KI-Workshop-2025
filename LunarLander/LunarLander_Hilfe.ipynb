{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6e93d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space Shape (8,)\n",
      "Sample observation [-72.297035    63.397247     3.1799192    2.2919981   -0.7411797\n",
      "  -3.2128975    0.6818337    0.46856788]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym \n",
    "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "print(\"Observation Space Shape\", env.observation_space.shape)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd7832",
   "metadata": {},
   "source": [
    "Wir sehen anhand von `Observation Space Shape (8,)`, dass die Beobachtung ein Vektor der Gr√∂√üe 8 ist, wobei jeder Wert unterschiedliche Informationen √ºber die Landeeinheit enth√§lt:\n",
    "\n",
    "- Horizontale Koordinate (x)\n",
    "- Vertikale Koordinate (y)\n",
    "- Horizontale Geschwindigkeit (x)\n",
    "- Vertikale Geschwindigkeit (y)\n",
    "- Winkel\n",
    "- Winkelgeschwindigkeit\n",
    "- Ob der Kontaktpunkt des linken Beins den Boden ber√ºhrt hat (boolean)\n",
    "- Ob der Kontaktpunkt des rechten Beins den Boden ber√ºhrt hat (boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7cc445a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Shape 4\n",
      "Action Space Sample 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bde2328",
   "metadata": {},
   "source": [
    "Der Aktionsraum (die Menge der m√∂glichen Aktionen, die der Agent ausf√ºhren kann) ist diskret mit 4 verf√ºgbaren Aktionen üéÆ:\n",
    "\n",
    "- Aktion 0: Nichts tun  \n",
    "- Aktion 1: Linkes Orientierungstriebwerk z√ºnden  \n",
    "- Aktion 2: Haupttriebwerk z√ºnden  \n",
    "- Aktion 3: Rechtes Orientierungstriebwerk z√ºnden  \n",
    "\n",
    "Belohnungsfunktion (die Funktion, die bei jedem Zeitschritt eine Belohnung vergibt) üí∞:\n",
    "\n",
    "Nach jedem Schritt wird eine Belohnung gew√§hrt. Die Gesamtbelohnung einer Episode ist die **Summe der Belohnungen aller Schritte innerhalb dieser Episode**.\n",
    "\n",
    "F√ºr jeden Schritt wird die Belohnung:\n",
    "\n",
    "- Erh√∂ht/verringert, je n√§her/weiter sich der Lander an der Landefl√§che befindet.  \n",
    "- Erh√∂ht/verringert, je langsamer/schneller sich der Lander bewegt.  \n",
    "- Verringert, je st√§rker der Lander geneigt ist (Winkel nicht horizontal).  \n",
    "- Um 10 Punkte erh√∂ht f√ºr jedes Bein, das Bodenkontakt hat.  \n",
    "- Um 0,03 Punkte verringert pro Frame, in dem ein Seitenantrieb z√ºndet.  \n",
    "- Um 0,3 Punkte verringert pro Frame, in dem das Haupttriebwerk z√ºndet.\n",
    "\n",
    "Die Episode erh√§lt eine **zus√§tzliche Belohnung von ‚Äì100 bzw. +100 Punkten** f√ºr Absturz bzw. erfolgreiche Landung.\n",
    "\n",
    "Eine Episode gilt als **L√∂sung**, wenn sie mindestens 200 Punkte erzielt.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
